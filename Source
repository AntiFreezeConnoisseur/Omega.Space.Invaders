#necessary contingencies to run atari and AI

import gym
import numpy as np

from stable_baselines.common.policies import MlpPolicy
from stable_baselines import PPO2

env = gym.make('SpaceInvaders-v0')

def evaluate(model, num_episodes=100):
  """Evaluate a PPO model.

  Args:
    model: (BaseRLModel object) the RL Agent.
    num_episodes: (int) number of time episodes to evaluate the model.
  Returns:
    (float) Mean reward for all the episodes
  """
  print("Start model evaluation.")
  all_rewards = []

  for i in range(num_episodes):
    obs = env.reset()
    done = False
    episode_reward = 0

    while not done:
      action, _ = model.predict(obs)
      obs, reward, done, info = env.step(action)
      episode_reward += reward

    print("Reward for episode {} is {}.".format(i+1, episode_reward))
    all_rewards.append(episode_reward)

  # Compute mean reward for the `num_episodes` episodes
  mean_reward = round(np.mean(all_rewards))
  print("Mean reward:", mean_reward, "Num episodes:", num_episodes)
  return mean_reward

model = PPO2('MlpPolicy', env, verbose=1)
# Evaluate the untrained model.
print("Start evaluating the untrained model.")
evaluate(model, num_episodes=1)

# Uncommenting the code below will train a brand new model.
# Note that this can take a long time if you set a large total_timesteps value.
for i in range(10):
  model.learn(total_timesteps=200)
  model.save("ppo_space_invaders_model" + str(i))

# Uncomment the code below to load the saved model.
model = PPO2.load("ppo_space_invaders_model")

# Evaluate the trained model.
print("Start evaluating the trained model.")
evaluate(model, num_episodes=1)


# Roll out the trained model.

for i in range(10):
  obs = env.reset()
  done = False
  while not done:
    action, _ = model.predict(obs)
    obs, _ , done , _ = env.step(action)
    env.render()

#if com still run but won't see it

env.close()

